[![Astro Deployment](https://img.shields.io/badge/Deployed%20on-Astro.io-blue)](https://your-astro-deployment-url.com)

# ğŸš€ Airflow ETL Pipeline with Postgres and API Integration

## ğŸ“Œ Project Overview
This project demonstrates the creation of an **ETL (Extract, Transform, Load) pipeline** using **Apache Airflow**. The pipeline extracts data from an external API (NASAâ€™s Astronomy Picture of the Day â€“ APOD API), transforms it into a clean format, and loads it into a **Postgres database**. The entire workflow is orchestrated by Airflow, enabling scheduling, monitoring, and managing complex data pipelines.

The setup leverages **Docker** to run Airflow and Postgres as services, ensuring an isolated and reproducible environment. Airflowâ€™s powerful hooks and operators are used to streamline the ETL process efficiently.

---

## ğŸ”‘ Key Components

### âš¡ Airflow for Orchestration
- Defines, schedules, and monitors the ETL pipeline.
- Manages task dependencies to ensure sequential and reliable execution.
- Uses a **DAG (Directed Acyclic Graph)** to structure tasks such as data extraction, transformation, and loading.

### ğŸ—„ï¸ Postgres Database
- Stores the extracted and transformed data.
- Runs inside a Docker container for easy management and persistence with volumes.
- Interacts with Airflow via **PostgresHook** and **PostgresOperator**.

### ğŸŒŒ NASA API (APOD)
- Provides astronomy picture of the day data including **title, explanation, and image URL**.
- Data is fetched using Airflowâ€™s **SimpleHttpOperator**.

---

## ğŸ¯ Objectives
1. **Extract Data**  
   Fetch astronomy-related metadata from NASAâ€™s APOD API on a scheduled basis (daily).

2. **Transform Data**  
   Clean and process JSON response (e.g., title, explanation, url, date) into a database-ready format.

3. **Load Data into Postgres**  
   Insert transformed data into a PostgreSQL table. If the target table doesnâ€™t exist, it is created automatically as part of the DAG.

---

## ğŸ—ï¸ Architecture & Workflow
The ETL pipeline is orchestrated in Airflow using a DAG with the following stages:

### 1ï¸âƒ£ Extract (E)
- Uses **SimpleHttpOperator** to make GET requests to NASAâ€™s APOD API.
- Receives JSON response containing astronomy metadata.

### 2ï¸âƒ£ Transform (T)
- Processes JSON using Airflowâ€™s **TaskFlow API** (`@task` decorator).
- Extracts and formats required fields: `title`, `explanation`, `url`, `date`.

### 3ï¸âƒ£ Load (L)
- Loads the cleaned data into **Postgres** using **PostgresHook**.
- Automatically creates the required table if it doesnâ€™t exist.

---

## ğŸ“Š High-Level Architecture Diagram
```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Extract   â”‚  --->  â”‚  Transform  â”‚  --->  â”‚     Load       â”‚
          â”‚ (NASA API)  â”‚        â”‚ (Clean JSON)â”‚        â”‚ (Postgres DB) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack
- **Apache Airflow** â€“ Orchestration & Workflow Management  
- **Python** â€“ Data transformation logic  
- **PostgreSQL** â€“ Persistent Data Storage  
- **Docker** â€“ Containerized environment  
- **NASA APOD API** â€“ Data Source  

---

## ğŸŒŸ Deployment
This pipeline can be deployed locally using **Docker Compose** or on cloud platforms like **Astronomer (Astro.io)** for production-ready orchestration.

